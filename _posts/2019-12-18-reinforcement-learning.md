---
author: Titipata
layout: post
title: "จาก Reinforcement Learning จนมาเป็น Deep Reinforcement Learning (ฉบับพกพา)"
description: "ทำความรู้จักการเรียนรู้แบบเสริมกำลัง (reinforcement learning) ตั้งแต่เบื้องต้น จนมาเป็น Deep Reinforcement Learning ได้ในงานวิจัยปัจจุบัน"
tags: [reinforcement, learning, q-learning, sarsa]
image:
  feature: 
comments: true
share: true
image:
  feature: /post/rl/rl_to_drl.png
date: 2019-12-18 22:30:00
---

สวัสดีอีกครั้งกับ tupleblog ไม่ได้เขียนบล็อกซะนานเลย เห็นบล็อกที่แล้วเขียนไปเมื่อปีที่แล้วค่อนข้างตกใจนิดหน่อย

วันนี้เราจะมาทำความรู้จักกับการเรียนรู้แบบเสริมกำลังหรือ reinforcement learning กัน พอเข้าใจว่ามีคนเขียนหัวข้อนี้กันไปเยอะพอสมควรแล้ว
แต่ในบล็อกนี้เราจะมาเริ่มต้นกันตั้งแต่พื้นฐานดังต่อไปนี้

- reinforcement learning คืออะไร
- เราสามารถฟอร์มโจทย์ของ reinforcement learning ได้ยังไง
- ลองทำโจทย์ง่ายๆของ Grid world ว่าเราจะหาชุดคำสั่งที่ดีที่สุดได้ยังไง
- แล้วจาก discrete reinforcement learning แบบง่ายๆ มันจะนำไปสู่ continuous reinforcement learning ได้ยังไงนะ
- Continuous reinforcement learning โดยใช้ Deep neural network
- ตัวอย่างไลบรารี่ที่สร้าง environment สำหรับ reinforcement learning
- และปิดท้ายด้วยเทคนิคต่างๆที่เราไม่ได้กล่าวถึงในโพสต์นี้นะฮะ

ส่วนใครที่อยากจะอ่านแบบเต็มๆ จริงๆจังๆหลังจากอ่านโพสต์นี้ ลองไปอ่านกันได้ในหนังสือ "Reinforcement Learning, An Introduction" ซึ่งเขียนโดย Richard Sutton and Andrew Barto นะฮะ ต้องยอมรับว่าหนังสือนี่ยาวกว่า

# Reinforcement learning คืออะไร

Reinforcement learning เป็นวิธีการเรียนรู้แบบนึงที่โดยการเรียนรู้เกิดมาจากการปฎิสัมพันธ์ (interaction) ระหว่างผู้เรียนรู้ (agent) กับสื่งแวดล้อม (environment)

สมมติตัวอย่างง่ายๆของการเรียนรู้โดยใช้ reinforcement learning

- ถ้าเราเกิดเป็นน้องหมาและเราหิวพอดี เราอยากจะได้อาหาร สิ่งแรกที่เราทำนั่นก็คือหันไปดูว่ามีใครอยู่ใกล้ๆบ้างนะ โอ้! เจ้าของอยู่ใกล้ๆพอดีเลย เรายืนมือออกไป ร้องขออาหารกับเจ้าของ และหลังจากเราขอแล้วสิ่งที่เราได้กลับมาคืออาหารนั่นเอง
- การเล่น Blackjack สมมติเราเป็นผู้เล่นและอยากจะได้เงินมากที่สุดจากการเล่น สิ่งที่เราควรทำก็คือต้องพนันอย่างเหมาะสม ถ้าช่วงไหนที่ไพ่คะแนนต่ำๆออกไปเยอะ เราก็ควรจะแทงสูงขึ้นเป็นต้น (หรือเรียกว่าเทคนิคไฮ-โลนั่นเอง)
- การเล่น Counter Strike ถ้าเราเป็นผู้เล่น ก็ต้องสังเกตว่ามีศตรูอยู่ใกล้ๆหรือไม่ ถ้ามีศตรูเราต้องหันไปทางศตรูและกดยิง

ลองนึกถึงอีกกรณีที่เจ้าของน้องหมาบอกให้นั่งนิ่งๆ ถ้านั่งแล้วจะได้กินขนม แต่ถ้าน้องหมาเดินไปเดินมาก็จะอดกินนั่นเอง แต่ถ้าฝึกไปบ่อยๆ น้องหมาก็จะเข้าใจว่า อ๋อ ถ้าได้รับคำสั่งนี้จะต้องทำแบบนี้นี่เอง

การที่น้องหมาสำรวจสิ่งรอบตัวแล้วลองทำอะไรซักอย่างและได้ผลลัพธ์กลับมานั้น เป็นการเรียนรู้แบบนึงซึ่งเราสามารถเขียนเป็นสมการคณิตศาสตร์ได้ โดยใช้การเรียนรู้แบบ reinforcement learning
โดยความเจ๋งของ reinforcement learning ก็คือว่าถ้าเราลองสำรวจและทำอะไรซักอย่างกับโลกไปเรื่อยๆ จะทำให้เราเรียนรู้มากขึ้นเรื่อยๆได้ เช่นในกรณีของน้องหมานั้น เมื่อเริ่มเข้าใจว่าจะทำยังไงให้ได้อาหาร ต่อไปก็จะจัดการได้อย่างถูกต้อง เช่นเจ้าของสั่งให้นั่งเราก็ควรจะนั่งเป็นต้น

# มาฟอร์มโจทย์ของ Reinforcement learning กันดีกว่า

ก่อนที่เราจะแก้ปัญหาว่าเราจะเรียนรู้ได้ยังไง เราจะมาเขียนแผนผังกันดูว่าหน้าตาของโจทย์ที่เราจะแก้หน้าตาจะเป็นยังไงนะ

<figure><center>
  <img width="600" src="/images/post/rl/workflow.png" data-action="zoom"/>

  <figcaption>
    <a title="Reinforcement Learning Workflow">
      การเรียนรู้เริ้มต้นที่สำรวจว่าอยู่ที่ state ไหนแล้ว S_0 ตัดสินใจทำบางอย่าง A_0 และได้ผลลัพธ์กลับมา R_1 เป็นแบบนี้ไปเรื่อยๆ
    </a>
  </figcaption>
</center></figure>

ในแผนผังนี้เราเริ่มต้นโดยการกำหนดผู้เรียนรู้ (agent) โดย agent สามารถสำรวจได้ว่าอยู่ที่ไหนของ environment (state) จากนั้น agent สามารปฎิสัมพันธ์กับสิ่งแวดล้อมโดยใช้การกระทำบางอย่าง  (action)  หลังจากที่ใช้ action ไปแล้ว agent ก็จะได้ reward กลับมา

เราสมมติว่าเวลาในที่นี้เดินแบบไม่ต่อเนื่องคือเริ่มจาก \\(t = 1, 2, 3, ...\\) นั่นเอง ในกรณีที่ถ้าเราเป็นน้องหมา สิ่งที่เราต้องทำอย่างแรกคือลองสังเกตว่า ในที่นี้เราจะเรียกว่า state (\\(S\\)  โดย \\(S_0\\) หมายถึง state ของน้องหมา ณ เวลา \\(t = 0\\) นั่นเอง โดยในเวลานี้น้องหมาสามารถออกคำสั่งได้หนึ่งอย่าง (\\(A_0\\))เช่น ร้องขอข้าว นั่ง กลิ้ง เป็นต้น หลังจากน้องหมาออกคำสั่งไปเรียบร้อย ก็จะได้ผลลัพธ์กลับมา เช่นการได้ข้าวเป็นต้น \\(R_1\\) และเวลาก็จะเลื่อนไปเป็น \\(t = 1\\) เป็นแบบนี้ไปเรื่อยๆ

เพราะฉะนั้น เราจะเขียนลำดับของการทำงานของน้องหมาจนกระทั่งน้องหมาแก่ตายได้ประมาณนี้

$$S_0 A_0 R_1 S_1 A_1 \ldots R_T S_T$$

ถ้า \\(t\\) ของเรามีจำกัดเช่นจาก \\(t = 1, ..., T\\) เราจะเรียกว่า Episodic task ยกตัวอย่างเช่น การเราจำลองเกมที่เมื่อเก็บเหรียญทั้งหมดครบถือเป็นอันจบด่าน หรือถ้าในเกมไพ่ Blackjack ถ้าไพ่หมดกองก็ถึงว่าจบหนึ่งตา ในกรณีที่ \\(t\\) สามารถมีค่าเพิ่มไปเรื่อยๆไม่สิ้นสุด เช่นอาจจะเป็นราคาของ Bitcoin, ราคาหุ้น ที่มีค่าใหม่มาเรื่อยๆทุกเวลา เราจะเรียกว่า Continuous task


# Finite Markov Decision Process (MDP)

จะเห็นว่าการทำงานของแผนผังที่เราเล่าไปข้างต้น มีลำดับดังต่อไปนี้

- สังเกตว่าตัวเองอยู่ในสภาวะใด (state)
- ออกคำสั่ง (action)
- ได้ผลลัพธ์หรือคะแนน (reward)

เราเรียกโจทย์นี้อีกชื่อนึงว่า finite Markov Decision Process (MDP) ซึ่งเป็นโจทย์ของปัญหา reinforcement learning ที่เราต้องการจะแก้นี่เอง โดยสิ่งที่เราต้องการนั่นคือการที่ได้ผลลัพธ์ที่ดีที่สุดในตอนท้าย โดยการจะทำให้ได้ผลลัพธ์ที่ดีที่สุดเราสามารถทำได้โดยการหาชุดคำสั่ง (policy) หรือ คำสั่ง (action) ที่ดีที่สุดในแต่ละ state ที่เราอยู่

$$ \pi: S \rightarrow A$$

Policy \\(\pi\\) สามารถเป็นชุดคำสั่งที่เราเขียนขึ้นเอง (deterministic) หรือใช้ความน่าจะเป็นก็ได้ (stochastic) โดยในกรณีของน้องหมาสิ่งที่เราควรทำคือฟังคำสั่งของเจ้าของเพื่อให้ได้อาหารมากที่สุด หรือในกรณีของการเล่น Blackjack เราก็ต้องการได้เงินมากที่สุดก่อนที่จะหมดรอบของการแข่ง (เลือกว่าจะหยิบไพ่เพิมหรือไม่หยิบ) เป็นต้น

คำถามถัดไปคือ การหาชุดคำสั่งที่ดีที่สุด เราต้องทำยังไงนะ? ในหัวข้อต่อไปเราจะมาพูดถึงการหาชุดคำสั่งที่ดีที่สุด (optimal policy) โดยใช้วิธี Monte Carlo และ Temporal Difference (Sarsa, Q-learning) กัน

# หาชุดคำสั่งที่ดีที่สุดโดยใช้วิธี Monte Carlo 

ในหัวข้อนี้เราจะมาคุยกันว่าเราจะสามารถหาชุดคำสั่งที่ดีที่สุดในแต่ละสถานะที่เราอยู่ได้ยังไงนะ เราไม่สามารถหาชุดคำสั่งที่ดีที่สุดได้ในทันที แต่วิธีหนึ่งในการหาชุดคำสั่งที่ดีที่สุดคือการลองผิดลองถูกและทำให้ดีขึ้นในครั้งหน้านั่นเอง

เพื่อความง่ายต่อการเข้าใจเราจะยกตัวอย่างของการเดินในตารางก่อน โดยเราจะเห็นว่าโลกของเรามีแค่ 4 states เท่านั้น (1, 2, 3, 4) ซึ่งแทนตาราง (ซ้ายล่าง, ซ้ายบน, ขวาล่าง, ขวาบน) ตามลำดับ ส่วนทิศทางการเดินก็ไปได้คือ (ซ้าย, บน, ขวา, ล่าง) โดยเราจะให้ agent เริ่มต้นจาก state 1 (ซ้ายล่าง) และพยายามไปให้ถึง state 4 (ขวาล่าง) เป้าหมายคือเราอยากจะหาชุดคำสั่งของแต่ละ state ที่ทำให้เราได้คะแนนมากที่สุดตอนถึงจุดสุดท้ายนั่นเอง จะเห็นว่าถ้าเรายิ่งเดินผิดมากเท่าไหร่ ก็จะยิ่งอยู่ใน Grid นานกว่าเดิมและคะแนนลดลงเรื่อยๆ

<figure><center>
  <img width="600" src="/images/post/rl/example-grid.png" data-action="zoom"/>
  <figcaption>
    <a title="Grid World">
      Grid World Example
    </a>
  </figcaption>
</center></figure>

การที่เราจะเดินให้ถูกทางสิ่งที่เราทำได้คือการประมาณค่าของรางวัลที่เราจะได้เมื่อใช้ action ใน state ที่เราอยู่ให้ได้นั่นเอง เราเรียกการประมาณนี่ว่าการหา action value โดยถ้าเราหาค่าเหล่านี้สำหรับทุกรูปแบบของ state กับ action เราจะเรียกว่า Q-table หรือตารางที่ประมาณค่ารางวัลของ state และ action ยิ่่งเราประมาณค่าในตารางได้ดีเท่าไหร่ เราก็จะรู้ได้ว่าเราควรจะใช้ action ไหนเมื่ออยู่ใน state ที่เราอยู่ เช่นในกรณีของ Grid ที่เราจำลองขึ้นมา เมื่อเราอยู่ที่ state 1 เราก็ควรจะเดินขึ้นถึงจะเข้าไปใกล้จุดหมายมากที่สุด หรือถ้าเราอยู่ state ที่ 3 เราก็ควรจะเดินลงถึงจะดีที่สุด

การประมาณ Q-table ทำได้หลายวิธีมากๆ โดยวิธีพื้นฐานที่สุดใช้วิธี Monte Carlo โดยวิธีนี้เราทำได้โดยการสร้าง agent ขึ้นมาและให้ agent เดินไปจนถึงจุดสุดท้าย แล้วอัพเดท Q-table จากรางวัลที่ได่ในตอนท้าย เราจะมาดูว่าการอัพเดทนี้ อัพเดทอย่างไรด้านล่าง

## อัพเดทตารางระหว่าง state-action (Q-table)

อย่างที่กล่าวไปข้างต้น วิธีนี้เราใช้การจำลอง agent ขึ้นมา และให้ agent เดินไปจนถึงจุดสิ้นสุด และอัพเดท Q-table

<figure><center>
  <img width="600" src="/images/post/rl/monte-carlo.png" data-action="zoom"/>
  <figcaption>
    <a title="Monte Carlo Update">
      การอัพเดท Q-table โดยใช้วิธี Monte Carlo และการเลือกชุดคำสั่งโดยใช้เทคนิค epsilon-Greedy
    </a>
  </figcaption>
</center></figure>

สมการของการอัพเดทเป็นดังนี้

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (G_t - Q(S_t, A_t))$$

โดย \\(G_t\\) คือรางวัลหรือคะแนน (reward) ที่เราได้ตอนจบ episode หลังจากออกใช้ action นั้นๆไป จะเห็นว่าถ้าหลังจากใช้ action ไปแล้วเราได้คะแนนน้อย ค่า Q ใน state และ action นั้นๆก็จะลดลง แต่ถ้าเราได้คะแนนกลับมามากขึ้น ค่า Q ก็จะเพิ่มมากขึ้นนั่นเอง

## สุ่มชุดคำสั่งโดยใช้วิธี \\(\epsilon-Greedy\\)

บางครั้งค่า Q ที่เรามีอยู่ก็อาจจะไม่ใช้ Q ที่ดีที่สุดก็ได้ ในบางครั้ง action บางอย่างอาจจะทำให้ได้ reward มากกว่า เพราะฉะนั้นเราต้องการเผื่อความน่าจะเป็นไว้เล็กน้อยสำหรับที่จะใช้ action ที่เราไม่เคยใช้มาก่อนในบางครั้ง เทคนิคการเลือกชุดคำสั่งแบบนี้เรียกว่า e-Greedy คือเราเผื่อความน่าจะเป็น \\(\epsilon\\) สำหรับการใช้ชุดคำสั่งอื่นๆไว้นั่นเอง

# หาชุดคำสั่งที่ดีที่สุดโดยใช้วิธี Temporal Difference Learning

จะเห็นว่าข้อเสียหลักๆของวิธี Monte Carlo ก็คือว่ากว่าเราจะอัพเดท Q-table หรืออัพเดทชุดคำสั่งใหม่นั้น เราจะต้องจำลองสถานการณ์จากเริ่มต้นจนจบ (จบ episode) ถึงจะสามารถอัพเดท Q-table และหาชุดคำสั่งที่ดีกว่าเดิมได้ ในกรณีนี้ต้องใช้เวลาและเปลืองทรัพยากรนานมากๆกว่าเราจะได้ Q-table ที่ดีที่สุด ในกรณีของเกมบางเกมเช่นหมากรุก โกะ หรือเกมที่เราออกแบบ กว่าจะเล่นจนจบ episode ต้องใช้จำนวนครั้งเยอะมากๆ ดังนั้นวิธี Monte Carlo จึงไม่เหมาะสมกับการหา Q-table ที่การจำลองอาจใช้เวลานานมากๆ

แต่ไม่เป็นไรฮะ ความเจ๋งก็คือเราสามารถอัพเดท Q-table ได้ในทันทีหลังจากเราได้ reward กลับมาและย้ายไปอีก state นึง โดยไม่จำเป็นต้องรอให้จบ episode ถึงจะอัพเดท วิธีต่อไปนี้ก็คือ Sarsa learning และ Q-learning นี่เอง

## Sarsa learning

วิธีการอัพเดทค่า Q แบบนึงหลังจากเราออกคำสั่งไปแล้ว เรียกว่า Sarsa โดยเทคนิคนี้เราจะอัพเดทค่าของ Q ของ state และ action ที่มีอยู่จาก reward ที่เราได้หลังจากออกคำสั่งไปแล้ว  state-action-reward-state-action

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$$

## Q-learning (Sarsa-max learning)

จะเห็นว่าจาก Sarsa learning จริงๆแล้วเราไม่ต้องรอจนกระทั่งเราออก action ในเวลาถัดไปด้วยซ้ำ เราแค่ต้องการค่าที่เราใช้อัพเดทได้ทันที ซึ่ง

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in A} Q(S_{t+1}, a) - Q(S_t, A_t)) $$

<figure><center>
  <img width="600" src="/images/post/rl/example-q-update.png" data-action="zoom"/>

  <figcaption>
    <a title="Q-update SARSA">
      ตัวอย่างการอัพเดท Q-table โดยใช้ Sarsa และ Q-learning
    </a>
  </figcaption>
</center></figure>

ลองมาดูตัวอย่างการอัพเดท Q-table กัน สมมติว่าเรามี Q-table ข้างบนและชุดคำสั่งหน้าตาประมาณนี้ (state 1, ขึ้น, -1, state 2, ขวา) เราจะอัพเดทค่า Q อย่างไร

**คำตอบ** จะเห็นว่าเราได้ชุดคำสั่ง (state 1, ขึ้น) มา เพราะฉะนั้นเราจะสามารถอัพเดท Q-table ที่ตำแหน่งนั้นได้ ถ้าใช้ Sarsa ค่า Q จะกลายเป็น `Q(state 1, ขึ้น) = 5 + 0.1 * (-1 + 8 - 5) = 5 + 0.2 = 5.2`

ส่วนถ้าใช้ Q-learning, ค่า Q จะกลายเป็น `Q(state 1, ขึ้น) = 5 + 0.1 * (-1 + max([8, 9, 7, 8]) - 5) = 5 + 0.2 = 5.3` แทน นั่นเอง

และถ้าเราทำการอัพเดทไปเรื่อยๆจะพบว่าค่า Q ของทั้งตารางจะเข้าใกล้ค่าเฉลี่ยของรางวัลที่เราควรจะได้มากขึ้นเรื่อยๆจนใกล้เกือบจุดที่ดีที่สุดนั่นเอง

# Reinforcement Learning ใน Continuous Space

จากตัวอย่างข้างบน ผู้อ่านอาจจะคิดว่า โห แล้วในชีวิตจริงมันไม่เหมือนตัวอย่างของ Grid world นี่นา ถ้าเรานึกถึงภาพของหุ่นยนต์ทำความสะอาดในบ้าน เราสามารถมีชุดตัวอย่างของ state มากกว่าแค่ใน Grid world (ซ้ายล่าง, ซ้ายบน, ขวาล่าง, ขวาบน) แต่เป็นตำแหน่งที่หุ่นยนต์ทำความสะอาดอยู่ (0.1 เมตร, 0.3 เมตร), (0.2 เมตร, 0.5 เมตร) จากจุดเริ่มต้น เพราะฉะนั้นตำแหน่งที่หุ่นยนต์อยู่บนพื้นอาจจะมีเยอะมากจริงๆ

วิธีที่ง่ายที่สุดที่เราสามารถใช่เทคนิคที่เพิ่งคุยกันไปมาใช้ก็คือการแบ่ง continuous state space เป็นช่วงๆหรือการ discretization นั่นเอง เราสามารถแบ่งช่องยิ่งเล็กก็จะยิ่งประมาณ continuous space ได้ดีขึ้น แต่ข้อเสียก็คือว่าขนาดของ Q-table ที่เราต้องสร้างก็จะใหญ่ขึ้นไปอีก และเมื่อ Q-table มีขนาดใหญ่มากๆ ก็อาจจะอัพเดทได้ไม่ทั่วถึงนั่นเอง

ยังมีอีกหลายวิธีที่เราสามารถแทนตำแหน่งใน continuous space ด้วยตัวเลขที่น้อยกว่าพิกัดจริงๆ เช่น การวาดวงกลมหลายๆวงไปบนพื้นผิว แล้วแทนตำแหน่งด้วยวงกลมที่ agent อยู่ แต่ถึงอย่างไรเราก็ต้องกำหนดขนาดของวงกลม กำหนดระยะการซ้อนและอีกหลายปัจจัย ซึ่งต้องใช้ความรู้พื้นฐานถึงจะออกแบบได้ดี

<figure><center>
  <img width="600" src="/images/post/rl/discretize-grid-world.png" data-action="zoom"/>

  <figcaption>
    <a title="Q-update SARSA">
      ตัวอย่างการแบ่ง continuous state space เป็นช่วงๆหรือเรียกว่า discretization
    </a>
  </figcaption>
</center></figure>

## Deep Q-Network (DQN)

แน่นอนว่าการแบ่ง state space เป็นตารางสามารถแก้ปัญหาที่มีขนาดจำกัดได้แต่เมื่อขนาดของ state เริ่มเป็นไปได้หลายรูปแบบ การใช้เทคนิคการแบ่งช่วงอาจจะทำไม่ได้ง่ายดาย ด้วยประเด็นนี้นี่เองที่ทำให้ deep neural network ซึ่งสามารถประมาณฟังก์ชั่นได้ในหลายรูปแบบมีส่วนเข้ามาทำให้การประมาณฟังก์ชันระหว่าง state และ action เป็นไปได้ง่ายขึ้นโดยไม่ต้องคำนวณหา Q-table เลย

โดยจุดเริ่มต้นของโมเดลที่มีชื่อเสียง ได้แก่ Deep Q-Network สร้างโดย Google Deepmind ที่สามารถเล่นเกม Pong ได้เก่งกว่าคนซะอีก โดย Deep Q-Network ใช้การประมาณ​ฟังก์ชั่นระหว่าง state ไปยัง action โดยตรงด้วย neural network เราจึงไม่ต้องพึ่งการเขียน Q-table อีกต่อไป การประมาณนี้อาจจะใช้ neural network ทั่วไปหลายๆชั้น หรืออาจจะเป็น convolutional neural network อย่างในเกม Pong ของ Atari ก็ได้ ในโพสต์หน้าๆเราจะมาลงรายละเอียดของ Deep Q-Network กัน

# ตัวอย่างของ Environments

พูดมาตั้งเยอะแยะ ผู้อ่านคงคิดว่านี่แต่ละครั้งเราต้องไปเขียน environment แล้วยังต้องเขียน agent เพื่อใช้แก้ปัญหาอีกหรอเนี่ย บอกเลยว่าตอนนี้มีเครื่องมือมากมายที่ช่วยให้เราเขียน environment ขึ้นมาเพื่อลองใช้ reinforcement แก้ปัญหาเพียบเลย ไม่จำเป็นต้องเขียน environment เองครับ

Open source สองอันที่คนใช้เป็นจำนวนมากเพื่อนำ environment ที่สร้างไว้เรียบร้อบมาลองเขียน agent ของตัวเองได้แก่ `gym` ที่เขียนโดย OpenAI และ `ml-agents` ที่เขียนโดย Unity ซึ่งสองโปรแกรมนี้ได้เขียนขึ้นมาโดยเราสามารถเรียก environment และ agent ขึ้นมาได้อย่างไม่ยากนัก ยกตัวอย่างเช่น `gym` ก็มี environment ของเกมไพ่อยาก Blackjack และ Cartpole ที่เราต้องพยายามพยุงไม้บนรถที่เลื่อนในสองมิติให้ได้

สำหรับ `gym` นั้น มาพร้อมกับ environment เกือบ 100 แบบที่สามารถให้เราลองแก้ปัญหาได้ และ `ml-agents` มาพร้อมกับ Unity Hub ซึ่งนอกจากเราจะสามารถใช้เกมที่มีอยู่ทั่วไปแล้ว ยังสามารถลองเขียนเกม
 (environment) ใหม่ๆพร้อมกับ agent และ action ได้อีกด้วย

ตัวอย่างของโค้ดในการสร้าง environment โดยใช้ไลบรารี่ `gym`

``` py
import gym
env = gym.make('Blackjack-v0')
print(env.action_space) # Discrete(2): hits (ขอไพ่เพิ่ม), sticks (ไม่ขอไพ่เพิ่ม)
observation = env.reset() # เริ่มดูไพ่
print(done) # จบตาแรก
action= env.action_space.sample() # สุ่ม action (hits/ sticks)
observation, reward, done, info = env.step(action)
print(done) # จบเทิร์น
```

และการสร้าง environment โดยใช้ไลบรารี่ `mlagents` ยกตัวอย่างเช่นเกมเก็บกล้วยของ Unity

``` py
from mlagents.envs.environment import UnityEnvironment
env = UnityEnvironment(file_name="/Banana_Linux_NoVis/Banana.x86_64")
brain_name = env.brain_names[0]
brain = env.brains[brain_name]

env_info = env.reset(train_mode=True)[brain_name]
print(env_info.agents) # number of agents = 1
print(brain.vector_action_space_size) # action size = 4 (w,a,s,d) => ขึ้น,ซ้าย,ลง,ขวา
prtin(len(env_infor.vector_observations)) # dimension of states = 37
```

เมื่อเราสร้าง environment ขึ้นมาแล้ว เราสามารถทดลอง

# สรุป

ในบล็อกนี้เราได้เรียนรู้กับ reinforcement learning โดยยกตัวอย่างของ Grid world เข้ามาเพื่อทำความเข้าใจกับการหา Q-table โดยใช้วิธี Monte Carlo และ Temporal Difference โดยใน continuous space เราไม่สามารถใช้ Q-table ได้เนื่องจากคาวมเป็นไปได้ของ state มีไม่จำกัด แต่เราสามารถใช้การแบ่ง Continuous space ให้เป็นช่วงๆเพื่อประมาณ Q-table ได้ (discretization) หรือเราจะเลือกใช้ neural network มาประมาณฟังก์ชันระหว่าง state กับ action เพื่อหาชุดคำสั่งก็ได้ เป็นที่มาของ Deep Reinforcement Learning นั่นเอง

ในโพสต์นี้เรายังไม่กล่าวถึง Policy agent และ Multi-agent Learning โดยเราจะเขียนเพิ่มเติมในโพสต์หน้าๆฮะ รอติดตาม!